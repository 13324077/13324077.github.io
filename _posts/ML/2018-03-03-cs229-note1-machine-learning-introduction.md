---
layout: post
title: 机器学习(1) - 线性回归
date: 2018-03-30 22:03:30 +08:00
category:
    - 机器学习
keywords: ML
tags:
    - 机器学习
---

# 监督学习介绍

监督学习的例子

对于如下数据集

| 居住面积(英尺) | 价格(1000美元)|
|:--------:|:-----:|
|2104|400|
|1600|330|
|2400|369|
|1416|232|
|3000|540|
|...|...|

47栋房子的面积与对应的价格，如下图所示

![living_area_with_price](/images/ml/living_area_with_price.png)

那么现在的问题是，从这个图中的点的分布，现在给定一栋房子的居住面积大小，如何预测它的价格呢？即房子的居住面积与价格之间的关系。

为了解决该问题，如下先介绍概念和标记法

**$$x^{(i)}$$**：表示"输入"变量（上面例子中的居住面积），也称为 **"输入特征"**。

**$$y^{(i)}$$**: 表示"输出"或目标变量（上面例子中的价格）。

**$$(x^{(i)}, y^{(i)})$$**: 称为 **训练样本**

**$$\{(x^{(i)}, y^{(i)}); i=1,...,m\}$$**: 称为 **训练集**, 上标$$(i)$$表示训练集中的对应训练样本的索引。

$$X$$： 通常表示输入值空间

$$Y$$： 通常表示输出值空间

上面例子中: $$X=Y=\Re$$


以下通过更加正式的方式描述监督学习问题，我们的目标是，给定 **训练集**， 学习一个函数 $$h:X \mapsto Y$$, 使$$h(x)$$是对相应的$$y$$的一个"好"的预测。该函数h称为"hypothesis"(不知道为什么这个称呼？)，过程如下图所示

![machine-learning-process](/images/ml/machine-learning-process.png)


当预测的 **目标变量** 是 **连续** 的， 那么该学习问题是一个 **回归问题**。

当预测的 **目标变量** 是 **离散** 的， 那么该学习问题是一个 **分类问题**。

以下从线性回归开始介绍

# 线性回归

对于如下数据集

| 居住面积(英尺) |卧室数| 价格(1000美元)|
|:--------:|:-----:|:-----:|
|2104|3|400|
|1600|3|330|
|2400|3|369|
|1416|2|232|
|3000|4|540|
|...|...|...|

这样 $$x$$就是一个二维的向量($$x\in \Re^{2}$$), 例如，$$x_1^{(i)}$$是训练集中第$$i$$栋房子的居住面积，$$x_2^{(i)}$$是第$$i$$栋房子的卧室数目。

> **注**:
>
> 一般来说，当设计一个学习算法解决问题时，首先需要决定选择哪些特征，所以，当收集房子的数据时，可能决定包括其他的一些特征，比如房子中是否有壁炉，浴室数目等等。
>
> 后面会介绍更多的关于 **特征选择**， 此处使用给定的特征

执行监督学习时，我们必须要决定如何在计算机中表示函数/假设 $$h$$， 此处作为初始选择，我们使用$$x$$的线性函数来近似y，如下：

$$h_{\theta}(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2$$

其中$$\theta_{i}$$是参数(也称为权值)， 用于对从$$X$$到$$Y$$映射的线性函数空间的参数化，当不会混淆时，可以去掉$$h_{\theta}(x)$$中的下标$$\theta$$，简化写为$$h(x)$$.

为了简化标记，引入约定的$$x_0 = 1$$(**截距**)，这样，上面的公式(1)简化写为

$$h(x) = \sum_{i=0}^n{\theta_i x_i} = \theta^{T}x$$

上式中的$$\theta^T x$$中的$$\theta$$和$$x$$都是 **向量**，$$n$$表示输入变量的个数($$x_0$$不计入内)

那么，现在给定一个训练集，如何得到或者学习到参数$$\theta$$呢？ 一个合理的方法就是在训练集上找到一个$$h(x)$$，让它尽可能的接近$$y$$，那么如何用数据公式来表达$$h(x)$$与$$y$$之间的接近程度呢？下面我们定义函数来度量在各个$$\theta$$值情况下，$$h(x)$$与$$y$$之间的接近程度，定义的函数称之为 **损失函数(cost function)**，如下

$$J(\theta) = \frac{1}{2} \sum_{i=1}^{m}{(h_{\theta}(x^{(i)}) - y^{(i)})}^2$$

即为 **最小二乘损失函数**， 从而产生了 **普通最小二乘回归模型**， 这只是非常大的算法簇中的一个特例。

## LMS算法

选择$$\theta$$使$$J(\theta)$$最小化，为此，使用搜索算法从初始的猜想的$$\theta$$开始，反复的改变$$\theta$$使$$J(\theta)$$逐渐变小，直到最后收敛到的$$\theta$$使$$J(\theta)$$最小。

以下考虑使用 **梯度下降算法**， 从初始的$$\theta$$开始，然后反复执行如下的更新:

$$\theta_{j} := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)$$

> **注**: 更新$$\theta$$时，是**同时**对所有的$$\theta_j (j = 0,...,n)$$值执行更新操作

其中， $$\alpha$$称为 **学习率**

梯度下降算法是一个很自然的算法，重复的在$$J$$最陡峭的下降方向进行$$\theta$$更新

从公式(4)可知，实现该算法，需要计算$$J(\theta)$$对$$\theta_j$$的偏导数，如下

$$
\begin{align}
\frac{\partial}{\partial \theta_j} J(\theta) &= \frac{\partial}{\partial \theta_j} \frac{1}{2} (h_\theta (x) - y)^2 \\
 &= 2 \cdot \frac{1}{2}(h_\theta(x) - y) \cdot \frac{\partial}{\partial \theta_j}(h_\theta (x) - y) \\
 &= (h_\theta (x) - y) \cdot \frac{\partial}{\partial \theta_j} \bigg(\sum_{i=0}^{n} \theta_i x_i - y \bigg) \\
 &=(h_\theta (x) - y)x_j
\end{align}
$$

因此，对于单个训练样本，$$\theta_j$$的更新规则如下:

$$
\theta_j := \theta_j + \alpha (y^{(i)} - h_\theta (x^{(i)})) x_j^{(i)}
$$

该规则称为 **LMS更新规则**, 也被称为 **Widrow-Hoff** 学习规则

> **LMS** ： Least Mean Squares， 最小均方

以上是单个训练样本的更新规则，对于训练集，则重复执行更新直到收敛，如下

<BR>
Repeat until convergence {

$$\qquad \theta_j := \theta_j + \alpha \sum_{i=1}^{m} (y^{(i)} - h_\theta (x^{(i)})) x_j^{(i)}$$ (对于每个j)

}

该方法在每一步都会查看整个训练集中每个例子，因此称为 **批量梯度下降(batch gradient descent)**

对应如下更新方式

$$Loop \{$$ $$\;$$

$$\qquad \;for \; i = 1 \;to\; m, \{$$ $$\;$$

$$\qquad\qquad\theta_j := \theta_j + \alpha (y^{(i)} - h_\theta (x^{(i)})) x_j^{(i)}$$  (对于每个j)

$$\qquad\}$$;

$$\}$$;

每次更新都是对一个训练样本进行更新， 该算法称为 **随机梯度下降 (stochastic gradient descent)**

## 正则方程

In this method, we will minimize $$J$$ by explicitly taking its derivatives with respect to the $$\theta_j$$’s, and setting them to zero


# 概率解释

对于回归问题，为什么是线性回归，尤其为什么最小均方损失函数$$J$$是一个合理的选择
