---
layout: post
title: 机器学习介绍
date: 2018-03-30 22:03:30 +08:00
category:
    - 机器学习
keywords: ML
tags:
    - 机器学习
---

# 监督学习介绍

监督学习的例子

对于如下数据集

| 居住面积(英尺) | 价格(1000美元)|
|:--------:|:-----:|
|2104|400|
|1600|330|
|2400|369|
|1416|232|
|3000|540|
|...|...|

47栋房子的面积与对应的价格，如下图所示

![living_area_with_price](/images/ml/living_area_with_price.png)

那么现在的问题是，从这个图中的点的分布，现在给定一栋房子的居住面积大小，如何预测它的价格呢？即房子的居住面积与价格之间的关系。

为了解决该问题，如下先介绍概念和标记法

**$$x^{(i)}$$**：表示"输入"变量（上面例子中的居住面积），也称为 **"输入特征"**。

**$$y^{(i)}$$**: 表示"输出"或目标变量（上面例子中的价格）。

**$$(x^{(i)}, y^{(i)})$$**: 称为 **训练样本**

**$$\{(x^{(i)}, y^{(i)}); i=1,...,m\}$$**: 称为 **训练集**, 上标$$(i)$$表示训练集中的对应训练样本的索引。

$$X$$： 通常表示输入值空间

$$Y$$： 通常表示输出值空间

上面例子中: $$X=Y=\Re$$


以下通过更加正式的方式描述监督学习问题，我们的目标是，给定 **训练集**， 学习一个函数 $$h:X \mapsto Y$$, 使$$h(x)$$是对相应的$$y$$的一个"好"的预测。该函数h称为"hypothesis"(不知道为什么这个称呼？)，过程如下图所示

![machine-learning-process](/images/ml/machine-learning-process.png)


当预测的 **目标变量** 是 **连续** 的， 那么该学习问题是一个 **回归问题**。

当预测的 **目标变量** 是 **离散** 的， 那么该学习问题是一个 **分类问题**。

以下从线性回归开始介绍

# 线性回归

对于如下数据集

| 居住面积(英尺) |卧室数| 价格(1000美元)|
|:--------:|:-----:|:-----:|
|2104|3|400|
|1600|3|330|
|2400|3|369|
|1416|2|232|
|3000|4|540|
|...|...|...|

这样 $$x$$就是一个二维的向量($$x\in \Re^{2}$$), 例如，$$x_1^{(i)}$$是训练集中第$$i$$栋房子的居住面积，$$x_2^{(i)}$$是第$$i$$栋房子的卧室数目。

> **注**:
>
> 一般来说，当设计一个学习算法解决问题时，首先需要决定选择哪些特征，所以，当收集房子的数据时，可能决定包括其他的一些特征，比如房子中是否有壁炉，浴室数目等等。
>
> 后面会介绍更多的关于 **特征选择**， 此处使用给定的特征

执行监督学习时，我们必须要决定如何在计算机中表示函数/假设 $$h$$， 此处作为初始选择，我们使用$$x$$的线性函数来近似y，如下：

$$h_{\theta}(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2$$

其中$$\theta_{i}$$是参数(也称为权值)， 用于对从$$X$$到$$Y$$映射的线性函数空间的参数化，当不会混淆时，可以去掉$$h_{\theta}(x)$$中的下标$$\theta$$，简化写为$$h(x)$$.

为了简化标记，引入约定的$$x_0 = 1$$(**截距**)，这样，上面的公式(1)简化写为

$$h(x) = \sum_{i=0}^n{\theta_i x_i} = \theta^{T}x$$

上式中的$$\theta^T x$$中的$$\theta$$和$$x$$都是 **向量**，$$n$$表示输入变量的个数($$x_0$$不计入内)

那么，现在给定一个训练集，如何得到或者学习到参数$$\theta$$呢？ 一个合理的方法就是在训练集上找到一个$$h(x)$$，让它尽可能的接近$$y$$，那么如何用数据公式来表达$$h(x)$$与$$y$$之间的接近程度呢？下面我们定义函数来度量在各个$$\theta$$值情况下，$$h(x)$$与$$y$$之间的接近程度，定义的函数称之为 **损失函数(cost function)**，如下

$$J(\theta) = \frac{1}{2} \sum_{i=1}^{m}{(h_{\theta}(x^{(i)}) - y^{(i)})}^2$$

即为 **最小二乘损失函数**， 从而产生了 **普通最小二乘回归模型**， 这只是非常大的算法簇中的一个特例。

## LMS算法
