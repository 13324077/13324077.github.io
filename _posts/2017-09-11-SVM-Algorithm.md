---
layout: post
title: SVM分类算法
date: 2017-09-11 17:16:30 +08:00
category: 机器学习
keywords: python, ML, SVM
tags: 机器学习
---

* content
{:toc}

## SVM相关概念

对于给定训练样本: $$D=\{(x_1, y_1), (x_2, y_2), ..., (x_m, y_m)\}, y_i \in \{+1, -1\}$$

分类学习最基本的想法就是基于训练集D在样本空间中找到一个划分超平面，将不同类别的样本分开，但能将训练样本分开的划分超平面可能有很多，如下图所示，如何确认哪一个是最优的呢？

![SVM_Example_show](/images/ml/SVM_Example_show.png)

从上图直观上看，应用去找位于两类训练样本"正中间"的划分超平面，即上图红色直线所示，因为该直线划分超平面对训练样本局部扰动的“容忍”性最好。

例如，由于训练集的局限性或噪声的因素，训练集外的样本可能比上图中的训练样本更接近两个类的分隔界。这将使许多划分超平面出现错误，而红色超平面受到的影响最小，也即这个红色划分超平面所产生的分类结果鲁棒性最好。对于未知示例的泛化能力最强。


在样本空间中，划分超平面通过如下线性方程表示

$$w^Tx + b = 0$$

其中：$$w = (w_1; w_2;...;w_d)$$为法向量，决定了超平面的方向，$$b$$为位移项，决定了超平面与原点之间的距离。由此可知，划分超平面可被法向量$$w$$和位移$$b$$确定，以下记为$$(w,b)$$。

样本空间中任意点$$x$$到超平面$$(w, b)$$的距离可写为

$$r = \frac{|w^T x+b|}{\|w\|}$$

假设超平面$$(w,b)$$能将训练样本正确分类，即对于$$(x_i, y_i) \in D$$，

若$$y_i = +1$$, 则有$$w^T x_i + b > 0$$

若$$y_i = -1$$, 则有$$w^T x_i + b < 0$$

令

$$
\begin{cases}
    w^T x_i + b \geq +1, & y_i=+1;\\
    w^T x_i + b \leq -1, & y_i=-1;
  \end{cases}
$$

如下图所示，

![SVM_margin_support_vector_show](/images/ml/SVM_margin_support_vector_show.png)

距离超平面最近的这几个训练样本点使上述等式成立，它们被称之为**“支持向量(support vector)”**，两个异类支持向量到超平面的距离之和为

$$
\gamma=\frac{2}{\|w\|}
$$

它被称之为**“间隔(margin)”**

为此，想要找到具有"最大间隔(maximum margin)"的划分超平面，就是能找到满足如下等式中约束的参数$$w$$和$$b$$, 使$$\gamma$$最大。

$$
\begin{cases}
    w^T x_i + b \geq +1, & y_i=+1;\\
    w^T x_i + b \leq -1, & y_i=-1;
  \end{cases}
$$


即

$$max_{w,b}\frac{2}{\|w\|} \\

\text{受约束于} y_i(w^T x_i + b) \geq 1, i = 1,2,...,m.

$$

因此，为了最大化间隔，仅需要最大化$${\|w\|} ^ {-1}$$，等价于最小化$${\|w\|}^{2}$$, 则上式重写为

$$
min_{w,b}\frac{1}{2}{\|w\|}^{2} \\

\text{受约束于} y_i(w^T x_i + b) \geq 1, i = 1,2,...,m.
$$

以上就是支持向量机(Support Vector Machine, SVM)的基本类型。


## 对偶问题

通过上述分析，我们希望通过求解

$$
min_{w,b}\frac{1}{2}{\|w\|}^{2} \\

\text{受约束于} y_i(w^T x_i + b) \geq 1, i = 1,2,...,m.
$$

得到最大间隔划分超平面对应的模型

$$f(x) = w^T x + b$$

其中$$w$$和$$b$$是模型参数。从公式$$min_{w,b}\frac{1}{2}{\|w\|}^{2}$$可知该问题是一个凸二次规划(convex quadratic programming)问题， 我们能直接通过现成的优化计算包求解。但是我们可以有更高效的方法。


对公式$$min_{w,b}\frac{1}{2}{\|w\|}^{2}$$使用拉格朗日乘子发可得到其"对偶问题(dual problem)",

具体来说，对公式$$min_{w,b}\frac{1}{2}{\|w\|}^{2}$$的每条约束添加拉格朗日乘子$$\alpha_i \geq 0$$, 则该问题的拉格朗日函数可写为

$$L(w,b,\alpha) = \frac{1}{2}{\|w\|}^{2} + \sum_{i=1}^{m}\alpha_i(1-y_i(w^T x_i + b)$$

其中
$$\alpha = (\alpha_1; \alpha_2; ... ;\alpha_m)$$,

令$$L(w,b,\alpha)$$对$$w$$和$$b$$的偏导为0,则

$$w = \sum_{i=1}^{m}\alpha_i y_i x_i$$

$$0 = \sum_{i=1}^{m}\alpha_i y_i$$

## 参考

内容来自与《机器学习 —— 周志华》
